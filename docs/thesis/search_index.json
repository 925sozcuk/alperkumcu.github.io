[["index.html", "Looking for Language in Space: Spatial Simulations in Memory for Language Preface 0.1 Abstract 0.2 Acknowledgement", " Looking for Language in Space: Spatial Simulations in Memory for Language Alper Kumcu July 2018 Preface 0.1 Abstract Grounded-embodied theories hold that language is understood and remembered through perceptual and motor simulations (i.e., activations and re-activations of sensorimotor experiences). This thesis aims to illustrate simulations of space in memory for language. In four experiments, we explored (1) how individuals encode and re-activate word locations and (2) how word meanings activate locations in space (e.g., bird - upward location). In the first part of the thesis (Experiment 1 and 2), we addressed the potential simulation of word locations by probing eye movements during memory retrieval (i.e., looking at nothing). In particular, we investigated why and when individuals need to rely on external memory support via simulation of word locations. Experiment 1 results reveal that the propensity to refer to the environment during retrieval correlates with individuals visuospatial memory capacity. That is, participants with worse visuospatial memory relied more on the environment; whereas, participants with better visuospatial memory relied more on the internal memory sources. Experiment 2 shows that words which are more difficult to remember and, particularly, words that are more difficult to visualise in mind lead to more reliance on the environment during word retrieval. Experiment 1 and 2 suggest that the opportunistic and efficient human mind switches between internal sources and external support as a function of the richness of internal sources and cognitive demands coming from the words to be remembered. The second part of the thesis (Experiment 3 and 4), focuses on spatial simulations triggered directly by words (i.e., language-based simulations). Experiment 3 is a norming study in which raters were asked to associate words with locations in space. Experiment 3 results demonstrate that there is a high degree of agreement among individuals when linking both concrete and abstract words to locations in space although there are no explicit conventions with regard to these associations. Ratings in Experiment 3 indicate potential locations of word-induced simulations. Normed words were used as stimuli in Experiment 4 in which recognition memory for words with spatial associations was probed. Experiment 4 results show that both language-based simulation of space and simulation of word locations dictate memory performance even if space is irrelevant and unnecessary for successful retrieval. In particular, words that were presented in incongruent locations as to the locations they imply (e.g., bird in a downward location) were remembered faster than words presented in congruent locations (e.g., bird in an upward location). Memory performance deteriorated whenever attention was shifted to the locations simulated with word meanings. Overall, the thesis specifies the mechanics of two different types of spatial simulation in language and their effects on memory. Results and their implications are discussed within the framework of grounded-embodied approaches to language and memory and the extended cognition. 0.2 Acknowledgement What a roller-coaster of emotions! The thesis you are reading now was made possible through scientific, mental and emotional support of many people, which makes me a very lucky researcher. Here, I would like to name a few, and I apologise in advance to all others that I forgot to mention. First and foremost, I would like to thank my supervisor Dr Robin L. Thompson for her never-ending support, meticulous supervision and invaluable insights. She shared my joy during success and she lifted me up during moments of anxiety (which I had a lot). Without her, I could not even imagine realising my dream of doing a PhD in psychology. Thank you for being a great person and a great supervisor. Language and Psychology Lab Group had a huge impact on me as a researcher and the present thesis. I would like to thank Professor Linda Wheeldon for never leaving my questions unanswered, Dr Steven Frisson for his contributions to the present work with his questions and constructive feedback, Dr Katrien Segaert for her support in mixed-effects modelling and for being a role model with her research and lively presentations, Dr Andrew Olson, Dr Andrea Krott and also Professor Sotaro Kita from University of Warwick for their helpful suggestions on the experiments reported in this thesis. My friends and colleagues here in UK deserve special thanks. Many thanks to Evi Argyriou for providing an inspiring example with her minimalist approach to research and presentation, Freya Atkins for being such a great (lab) mate, Mahmoud Elsherif for our fruitful debates about science and life in general, Isabella Fritz for our discussions in grounded-embodied cognition and R, Anneline Huck from University College London and Beinan Zhou for their support in collecting the pilot data, Linde Luijnenburg for her company at the British Library, Rachel Merchant for her support in recording the auditory stimuli, Kristian Suen for the teamwork in organising Language and Cognition Seminars (LanCoS) and Onur Usta for the library sessions and gallons of coffee. I would like to acknowledge Turkish Council of Higher Education and Hacettepe University for awarding me with a doctoral scholarship and funding my research. I am grateful to my colleagues and friends at Hacettepe University in Turkey; especially Dr Sinem Sancaktarolu Bozkurt, Dr Hilal Erkazanc Durmu, Dr Elif Ersözlü and Dr Sezen Ergin Zengin for smoothing out the funding process and for their lovely companionship. I would like to thank all the participants who kindly participated the experiments. It goes without saying that the research could not be conducted without them. I also would like to thank our omniscient post-grad administrator Parveen Chahal for answering millions of questions that I have asked during my years at the School of Psychology. Last but not least, I am deeply grateful to my mother and sister, who have sent their affection and support from miles away. The majority of the people I mentioned here is woman. A heartfelt appreciation to all women in science! "],["introduction.html", "1 Introduction 1.1 Statement of Research Question 1.2 Definition of Concepts 1.3 Overview of Thesis", " 1 Introduction 1.1 Statement of Research Question The link between space, language and memory is one of the most intriguing topics is cognitive psychology. Decades of evidence has showed that language provides us with a framework to materialise and structure the relatively abstract notion of space (Bloom, Garrett, Nadel, &amp; Peterson, 1996; Carlson-Radvansky &amp; Logan, 1997; Majid, Bowerman, Kita, Haun, &amp; Levinson, 2004; Miller &amp; Johnson-Laird, 1976; Talmy, 1983). Thus, language influences how people perceive, think about and remember space (Levinson, 2003). The blooming fields of grounded-embodied cognition and the extended mind thesis have redefined the space-language-memory network and rekindled an interest about space with a novel perspective. Once seen as a central processing machine contained in the skull, the mind is now viewed as more of an interactive architecture extending onto the body and space. In accordance, burgeoning evidence suggests that space is not only a content but also a medium for language and memory (Mix, Smith, &amp; Gasser, 2010). Memories can be indexed, and abstract thoughts can be grounded in space. Further, comprehending language can give rise to non-linguistic, spatial experiences. Despite the abundance of demonstrative experiments, how spatial perception and cognition influence language and memory operations is yet to be defined. To this end, the present thesis explores robust, systematic and often surprising ways that space is involved in memory for language. In particular, this thesis is an attempt to define and systematise different characteristics of spatial engagements during retrieval of words from memory. Thereby, it aims to contribute to the understanding of the effect of space on memory for language. 1.2 Definition of Concepts The key concepts examined in the present work are defined in this section. Thereby, the scope of the thesis is outlined. 1.2.1 Spatial indexing The human mind can anchor spatially-located information to external spatial locations. Simply put, the location of a visual item is encoded with the item itself. This behaviour is called as spatial indexing. Marr (1982) was among the first to argue that the visual system separates locations from visual features (what vs. where). He introduced the term place token to refer to representations of location. A place token indexes the locations of visual information at the early stages of visual processing. Pylyshyn (1989) operationalised the phenomenon of spatial indexing in an exhaustive model termed FINST. The model assumes that spatial indexing is a primitive, that is pre-attentive or a pre-cognitive mechanism, which precedes higher visual operations such as recognition of patterns. But importantly, spatial indices allow for stability of visual information by constructing stable representations of locations in a constantly changing visual world. Hence, an index keeps pointing to the same location even if the visual pattern moves across the retina. According to the model, therefore, spatial indexing is different from merely encoding the position of a feature because a spatial index makes it possible to locate the visual stimulus for further examination when the necessity arises (see also Ballard, Hayhoe, Pook, &amp; Rao, 1997). In a similar vein, Coslett (1999) proposes a spatial registration hypothesis. This hypothesis assumes that all stimuli, even if not relevant to the task at hand, are automatically marked with respect to spatial location in egocentric coordinate systems (p. 703). Registering a location entails the creation of a marker that specifies the coordinate of an object in relation with the other objects in the environment. According to spatial registering hypothesis, spatial indexing is limited with the capacity of visual attention. 1.2.2 Looking at nothing Consider the following situation: You are in the middle of a maths exam, trying to solve a problem. The problem you engage requires the application of a formula that you fail to remember at that point. However, you remember that the instructor has previously used the blackboard to explain the formula in one of the classes. You remember that she has written the formula on the top left corner of the board. Of course, the blackboard has already been cleaned and is totally blank now. Still, you raise your head in the hope of a sudden recall and look at the previous, but now-blank location of the formula. You have just looked at nothing (Spivey &amp; Geng, 2000). Obviously, there is nothing on the board. What you are looking at is the spatial indice that represents the previous but now-blank location of the formula. In this respect, looking at nothing is a memory-guided behaviour. Your eyes have been oriented to the previous location of the formula because you have attempted to remember it. To be more precise, spatial indices tied to external visual and verbal information trigger eye movements when a mental representation is reactivated. Thus, when retrieving information from memory, people tend to exploit location-based indices and look at the seemingly uninformative, empty locations where the information originally occurred even if the location is irrelevant to the task. Looking at nothing follows spatial indexing in a typical memory task with encoding and retrieval stages. Individuals are expected to index the location of the information at encoding. Then, they are expected to look at the previous locations of the to-be-retrieved information during retrieval. A considerable number of empirical studies has documented looking at nothing (e.g., Altmann, 2004; Richardson &amp; Kirkham, 2004; Richardson &amp; Spivey, 2000; Spivey &amp; Geng, 2000). The link between mental representations and looking behaviour (de Groot, Huettig, &amp; Olivers, 2016; Ferreira, Apel, &amp; Henderson, 2008; Martarelli, Chiquet, Laeng, &amp; Mast, 2017; ORegan, 1992; Renkewitz &amp; Jahn, 2012; D. C. Richardson, Altmann, Spivey, &amp; Hoover, 2009; D. C. Richardson &amp; Spivey, 2000; Scholz, Mehlhorn, &amp; Krems, 2011; Wantz, Martarelli, &amp; Mast, 2015) and whether looks to blank locations improve memory (Johansson, Holsanova, Dewhurst, &amp; Holmqvist, 2012; Johansson &amp; Johansson, 2014; Scholz, Klichowicz, &amp; Krems, 2018; Scholz, Mehlhorn, &amp; Krems, 2016) have received much attention in looking at nothing research. 1.2.3 Simulation In grounded-embodied cognition, a simulation is defined as a partial activation or reactivation of an original perceptual, motor, affective or introspective experience (Barsalou, 1999). A simulation can occur in the absence (offline) or upon the perception (online) of the original stimulus. A large body of neural and behavioural evidence indicates that major segments of cognition such as mental imagery, memory, language comprehension, consciousness, expertise and several cognitive performances such as facial mimicry, gesturing, reasoning and problem solving rely on simulations (see Dijkstra &amp; Post, 2015; Hesslow, 2011; Körner, Topolinski, &amp; Strack, 2016; Wilson, 2002 for reviews). The current thesis focuses on offline and online simulations of space in memory for language. Perceptual symbols lie at the heart of simulation mechanism as they make it possible the (re)activation of sensorimotor experiences (Barsalou, 1999). Perceptual symbols are mental representations that represent conceptual knowledge in the mind. Crucially, perceptual symbols are represented in the same system as the perceptual states that produced them (Barsalou, 1999, p. 578). As a result, they reflect physical and thus, perceptual characteristics of the referents they stand for, and represent continuous, rich and multimodal phenomenological experience. That said, perceptual symbols partly represent their referents rather than being similar to high-resolution video-clips or high-fidelity sound clips (Zwaan, Stanfield, &amp; Yaxley, 2002). Based on these features, perceptual symbols are fundamentally different from physical symbols (Newell, 1980) asserted within the computational theories of mind (Fodor, 1975; Haugeland, 1985; Newell &amp; Simon, 1976; Putnam, 1960). In contrast to perceptual symbols, physical symbols are amodal, abstract and discrete units. That is, they do not reflect the perceptual modality of or do not resemble to the physical entities that they refer to as in 1 and 0s in a computation environment (Harnad, 1990; Pylyshyn, 1986). 1.2.3.1 Offline simulation Consider that you need to remember a piece of conceptual information about cats (e.g., What does a cat sound like?). According to grounded-embodied cognition, in such a case, the human mind relies on the reactivation of perceptual, motor, affective or introspective experiences that are formed during previous interactions with a cat (e.g., how soft its fur is, how it smells, how you feel when you stroke it etc.). However, these experiences or states are not reinstated exactly on later occasions and different contexts may distort activations of the original representations (Barsalou, 1999, p. 584). Offline simulations have a situated character (Barsalou, 2003). For instance, they represent specific cats in specific situations rather than representing generic knowledge about cats. You might have noticed that remembering a piece of information about cats in the way described above bear similarities with remembering the location of a maths formula. In the latter case, the individual encodes conceptual information (the formula) along with a spatio-perceptual experience (perceiving the location of the formula on the blackboard). Later, when she needs to access the conceptual information through memory retrieval, she reactivates the perceptual experience associated with it. In other words, she re-lives the perceptual experience that she has during the encoding of the conceptual information. On this ground, looking at nothing can be understood as a reflection of spatial simulation that takes place in the absence of the original stimulus. 1.2.3.2 Online simulation Perceptual, motor, affective or introspective experiences can be activated whenever an individual perceives a stimulus. For example, merely viewing a graspable object such as a cup, a knife or a frying pan simulates the potential act of grasping. In turn, brain regions associated with motor movements are activated (Chao &amp; Martin, 2000; Tucker &amp; Ellis, 1998). Similarly, remembering a Japanese kanji character stimulates motor activity in the areas that would be activated when actually writing the characters (Kato et al., 1999; Topolinski &amp; Strack, 2009). Merely listening to words that involve strong tongue movements when pronounced such as birra (beer in Italian) or ferro (iron in Italian) activates tongue muscles (Fadiga, Craighero, Buccino, &amp; Rizzolatti, 2002). Language comprehension gives rise to simulations in this manner (see Chapter 2.1.3). To summarise, the fundamental difference between an offline and an online simulation is the existence of an external stimulus at the time of the simulation. An offline simulation is a recreation of previous sensorimotor activations without any stimulus when the stimulus is re-accessed. An online simulation is a sensorimotor activation upon the perception of a stimulus (see Chapter 2.1 for further clarification with examples). There are other instances of a simulation mechanism in cognitive psychology. In social cognition, for instance, attributing mental states to others as in mind reading (i.e., theory of mind) (Gallese &amp; Goldman, 1998; Premack &amp; Woodruff, 1978) is thought to be a simulation based on mirror neurons that are activated merely by observing others (Caggiano et al., 1996; Gallese, Fadiga, Fogassi, &amp; Rizzolatti, 1996). This aspect of simulations is beyond the scope of the current thesis despite the possible overlaps between theory of mind and mental simulations (Shanton &amp; Goldman, 2010). Likewise, simulation within the context of this thesis does not refer to computer-based simulations. 1.2.4 Cognitive offloading Cognitive agents can perform physical actions to outsource their cognitive work to the body and the world with the aim of reducing cognitive load (Wilson, 2002). This behaviour is called as cognitive offloading (Risko &amp; Gilbert, 2016). Making a shopping list instead of keeping items to buy in your mind is a typical example of such behaviour. Cognitive offloading is treated as a fundamental mechanism of extended cognition, which posits that there is not a strict border between the mind and the world with the mind leaking into the world in surprising ways (A. Clark, 1998; A. Clark &amp; Chalmers, 1998). Cognition can be extended to different body parts during several cognitive problems such as using hands in finger-counting (Butterworth, 2005) and co-speech gestures in communication (see Goldin-Meadow &amp; Wagner, 2005; Pouw, de Nooijer, van Gog, Zwaan, &amp; Paas, 2014 for reviews). Cognitive work can be offloaded directly onto the world as well. In particular, people can exploit their immediate space in an intelligent way to reduce their cognitive load. In such a case, space becomes a resource that must be managed, much like time or energy (Kirsh, 1995). For example, Kirsh and Maglio (1994) showed that in Tetris, participants solve perceptual and cognitive problems (e.g., judging whether a geometric piece can fit into a specific position) in the space by physically rotating the pieces on the screen rather than in their minds which would require mental rotation. Cognitive offloading with eye movements is a special case in which both body (via eye movements) and environment are used at the same time. In a seminal study, for instance, Hayhoe, Bensinger and Ballard (1998) asked participants to copy a pattern of coloured blocks on a computer screen. Participants had to use a mouse to drag scrambled blocks from the source window to an empty workspace window. The colour of blocks in the pattern was changed at different points during the task. Results showed that participants launched frequent fixations to the original pattern when they were copying it into the workspace window rather than keeping the original pattern in mind. In this respect, looking at nothing can be conceptualised as a type of cognitive offloading in which eyes are used to index certain locations in space for subsequent use in line with information-gathering goals (Ballard et al., 1997). There are two important aspects of cognitive offloading. First, cognitive offloading often results in betterment in performance. For example, children were found to be most accurate when they actively gesture to count (even when using a puppet to count for them) as compared to internal counting (Alibali &amp; Dirusso, 1999). Second, exploitation of body and space to reduce cognitive load is correlated with the cognitive demands coming from the task and internal, biologic cognitive capacity. That is, higher cognitive load and/or lower cognitive capacity result in more frequent cognitive offloading. For example, in Risko and Dunn (2015), participants were asked to remember a mixture of random letters in a traditional short-term memory task. They either had to rely on internal memory only or had the option to write down the presented information and thus, to externalise memory work. Results showed that use of external storage became more frequent as the number of letters in a string increased. Further, individual short-term memory capacity predicted the likelihood of writing down the to-be-remembered information. The functional role of cognitive offloading and the link between cognitive load/capacity and the tendency of offloading indicate that there is a systematic trade-off between internal and external processing (Schönpflug, 1986). In the face of costs of cognitive operations and limited cognitive sources, a successful cognitive agent seeks to get the job done in the easiest way possible by integrating internal with external processes in complex environments (A. Clark, 1989). 1.3 Overview of Thesis The current thesis investigates activations and re-activations of space in memory for language. It contains seven chapters (including this one). Chapter 2 includes the review of the literature. Chapter 2.1 focuses on the simulations in mental imagery, memory and language as the core mechanism of grounded-embodied cognition. Chapter 2.2 addresses the relation between eye movements and memory. Chapter 3 contains the first experimental study in the form of a paper. This study investigates whether individuals with better visuospatial memory relies more on space and simulation of word locations through looks at previous but now-blank locations when retrieving words from memory. Chapter 4 contains the second experimental study in the form of a paper. This study employs the experimental design developed in Chapter 3 to investigate whether words that are more difficult to maintain and retrieve from memory lead to more reliance on space and spatial simulations via looking at nothing. It also examines the contributions of word properties (e.g., word length, imageability, frequency etc.) to the looking behaviour and memory performance. Chapter 5 contains the third experimental study in the form of an article. This is a lab-based norming study in which participants were asked to read 1439 concrete and abstract words and associate them with horizontal and vertical locations on a two-dimensional coordinate system. Chapter 6 contains the forth experimental study in the form of a paper. This study uses spatially normed words from Chapter 5 to investigate how spatial locations suggested and simulated by word meanings affect recognition memory performance in relation with the physical locations of the words on the screen. Finally, Chapter 7 (General Discussion) summarises findings and conclusions of the empirical studies and discusses them in the context of the grounded-embodied and extended approaches to memory and language. Additionally, it includes suggestions for future work. "],["theoretical-background.html", "2 Theoretical Background 2.1 Mind, Recreated: Simulations in Imagery, Memory, and Language 2.2 I Look, Therefore I Remember: Eye Movements and Memory", " 2 Theoretical Background 2.1 Mind, Recreated: Simulations in Imagery, Memory, and Language 2.1.1 Mental imagery as a simulation Mental imagery is the ability to construct mental representations in the absence of external sensory stimulation. Thus, it is a quasi-phenomenal experience (N. J. T. Thomas, 1999, 2018a). That is, it resembles the actual perceptual experience but occurs when the appropriate external stimulus is not there. The ability to see with the minds eye without any sensory stimulation is a remarkable feature of the human mind. Mental imagery underlies our ability to think, plan, re-analyse past events or even fantasise events that may never happen (Pearson &amp; Kosslyn, 2013). Accordingly, mental images involve, alter or even replace the core operations of human cognition such as memory (Albers, Kok, Toni, Dijkerman, &amp; De Lange, 2013; Rebecca Keogh &amp; Pearson, 2011; Tong, 2013), problem-solving (Kozhevnikov, Motes, &amp; Hegarty, 2007) decision-making (Tuan Pham, Meyvis, &amp; Zhou, 2001), counter-factual thinking (Kulakova, Aichhorn, Schurz, Kronbichler, &amp; Perner, 2013), reasoning (Hegarty, 2004; Knauff, Fangmeier, Ruff, &amp; Johnson-Laird, 2003), numerical cognition (Dehaene, Bossini, &amp; Giraux, 1993) and creativity (LeBoutillier &amp; Marks, 2003; Palmiero et al., 2016). The human mind can mentally visualise not only visual but also nonvisual perceptions (Lacey &amp; Lawson, 2013) such as auditory mental imagery (e.g., imagining the voice of a friend or a song) (Lima et al., 2015) or motor mental imagery (e.g., mentally rehearsing a movement before actualising it) (Hanakawa, 2016). Mental images can arise from nonvisual modalities (particularly auditory or haptic) in congenitally blind individuals (Cattaneo et al., 2008). However, the literature of mental imagery is largely dedicated to metal imagery that is specifically visual (Tye, 1991). How do we imagine? By extension, what does a mental image look like? The format of mental images has been extensively discussed in the 70s and 80s with two camps: pictorial (depictivism) and propositional (descriptivism) imagery. The pictorial position (Kosslyn, 1973) holds that mental images are like pictures and there are spatial relations between the imagined objects. On the other hand, the propositional view (Pylyshyn, 1973) is that mental images are more like linguistic descriptions of visual scenes based on tacit knowledge about the world (i.e., implicit knowledge that is difficult to express explicitly such as the ability to ride a bike). Mental imagery under the treatment of descriptivism is more of an amodal, formal system. Whereas, depictivism offers a picture of mental imagery that appears more compatible with the mechanics of grounded-embodied cognition. However, neither of these approaches captures the true essence of grounded-embodied cognition because both of them depend on an information processing approach. In both cases, perceptual data flows inward to a passive cognitive agent (N. J. T. Thomas, 1999). On the other hand, grounded-embodied theories of cognition conceive that mental imagery is based on active perceptions and actions. Mental images are considered as mental representations reactivated through previous perceptions (Ballard et al., 1997). Consequently, mental imagery is a simulation itself (Barsalou, 1999). As a matter of fact, mental imagery is assumed to be the most typical example of the simulation mechanism in that there are certain similarities between the properties of a sensorimotor simulation and mental imagery (Markman, Klein, &amp; Suhr, 2008): First, mental images arise from perceptual representations. They are formed in the absence of the original perceptual stimulation. And lastly, a mental image is not an exact copy of the percept but rather, a partial recreation (Kosslyn, 1980). Within this view, the primary function of mental imagery is to simulate reality at will in order to access previous knowledge and predict the future (i.e., mental emulation) (Moulton &amp; Kosslyn, 2009). In order to verify that mental imagery is sensorimotor simulation, evidence showing similarities between perception and imagery is needed. This is indeed what the literature on mental imagery within the framework of grounded-embodied cognition indicates. For instance, an overwhelming body of neuroimaging evidence shows that similar brain regions are activated during perception and imagery stages (Cichy, Heinzle, &amp; Haynes, 2012; Ganis, Thompson, &amp; Kosslyn, 2004; Ishai &amp; Sagi, 1995; Kosslyn, Thompson, &amp; Alpert, 1997; OCraven &amp; Kanwisher, 2000). Behavioural studies further reveal the nature of the link between perception and imagery. As early as 1910, the psychologist Cheves Perkys experiments showed that visual mental images can supress perceiving real visual targets unconsciously (i.e., the Perky effect) (Craver-Lemley &amp; Reeves, 1992; Perky, 1910). In the original experiment, participants were asked to fixate a point on a white screen and visually imagine certain objects there such as a tomato, a book or a pencil etc. After a few trials, a real but a faint image (i.e., in soft focus) of the concerned object was projected onto the screen. Participants failed to distinguish between their imagined projections and the real percepts. Shortly, real images intermingled with the mental images. For instance, some participants reported their surprise when they imagined an upright banana rather than a horizontally oriented one they were attempting to imagine (N. J. T. Thomas, 2018b). The Perky effect indicates that mental imagery and visual perception draw on the same sources (see also Finke, 1980). In a similar study (Lloyd-Jones &amp; Vernon, 2003), participants saw a word (e.g., dog) accompanied by a line drawing of that object in the perception phase. In the imagery phase, participants made spatial judgements about the previously shown picture. Simultaneously, a picture distractor appeared on the screen during mental imagery. The picture distractor was either unrelated to the mental image of the previously shown object (e.g., dog - strawberry) or conceptually related (e.g., dog - cat). Response times in the judgement task were longer when participants generated a mental picture along with the perception of a conceptually related picture but not a conceptually unrelated picture. These findings suggest that imagery and visual perception share the same semantic representations. Mental images are also processed in similar ways as the actual images. In Borst and Kosslyn (2008), participants scanned a pattern of dots and then, an arrow was shown on the screen. Participants then decided whether the arrow pointed at a location that had been previously occupied by one of the dots. Results showed that the time to scan during imagery increased linearly as the distance between the arrow and the dots increased in perception. Further, participants who were better at scanning distances perceptually were also better at scanning distances across a mental image, suggesting the functional role of perception in mental imagery. Finally, eye movement studies have given considerable support to the simulation account of mental imagery with two key findings (see Laeng, Bloem, DAscenzo, &amp; Tommasi, 2014 for a review): First, eye movements during perception are similar to those during imagery (Brandt &amp; Stark, 1997; Johansson, Holsanova, &amp; Holmqvist, 2006). Second, the amount of overlap between eye movements during perception and imagery predicts the performance in imagery-related tasks (Laeng &amp; Teodorescu, 2002). Eye movements in mental imagery are further elaborated in Chapter 2.2.3. 2.1.2 Mental retrieval as a simulation A simulation account of memory views memory retrieval as a partial recreation of the past that often includes sensorimotor and contextual details of the original episode (see Buckner &amp; Wheeler, 2001; Christophel, Klink, Spitzer, Roelfsema, &amp; Haynes, 2017; Danker &amp; Anderson, 2010; Kent &amp; Lamberts, 2008; Pasternak &amp; Greenlee, 2005; Rugg, Johnson, Park, &amp; Uncapher, 2008; Xue, 2018 for comprehensive reviews and see De Brigard, 2014; Mahr &amp; Csibra, 2018; Marr, 1971 for theoretical discussions). Hence, memory retrieval can be thought as a simulation of encoding in a similar way to mental imagery being a simulation of perception. Indeed, it is known that mental imagery and memory operate on similar machinery as long as their perceptual modalities match (e.g., visual mental imagery - visual memory). In the original model of working memory, Baddeley and Hitch (1974) assumed that one function of the visuospatial sketchpad (i.e., the component of working memory responsible for the manipulation of visual information) is manipulating visual mental images. In support of this assumption, Baddeley and Andrade (2000) showed that visual and auditory mental imagery tasks disrupted visual and auditory components of working memory respectively; that is, visuospatial sketchpad and phonological loop (i.e., the component of working memory responsible for the manipulation of auditory information). Keogh and Pearson (2014) evidenced that individuals with stronger visual mental imagery also have greater visual working memory capacity but not verbal memory capacity (see also Keogh &amp; Pearson, 2011). Grounded-embodied cognition takes the link between mental imagery and memory one step forward: Memory not only involves mental imagery, but memory is mental imagery itself. In accordance, encoding corresponds to perception and retrieval corresponds to imagery. In this respect, Albers et al. (2013) presented strong evidence that working memory and mental imagery share representations in the early visual cortex (V1 - V3). Further, as Buckner and Wheeler (2001) noted assessments of visual mental imagery ability in patients with damage to visual cortex support the possibility that brain regions involved in perception are also used during imagery and remembering (De Renzi &amp; Spinnler, 1967; D. N. Levine, Warach, &amp; Farah, 1985). Mental time travel is a striking example of the role of imagery in memory (Corballis, 2009; Schacter, Addis, &amp; Buckner, 2007; Suddendorf &amp; Corballis, 2007; Szpunar, 2010). Mental time travel is a cognitive ability of episodic memory (i.e., conscious and explicit recollection of past events) and episodic future thinking through mental imagery. Thus, a mental time traveller can mentally project herself backwards in time to re-live (i.e., reconstruct) the past events and pre-live (i.e., predict) the possible future events (Suddendorf &amp; Corballis, 2007). In this respect, mental time travel can be considered as an intertemporal simulation (Shanton &amp; Goldman, 2010). Growing evidence has shown that episodic memory and simulation of future by mental imagery share a core neural network (i.e., default network) (see Schacter et al., 2012 for a review), suggesting that memory, mental imagery and thinking about future rest on the similar neural mechanisms. As in mental imagery, a simulation approach to memory underlines the correspondence between encoding and retrieval (Kent &amp; Lamberts, 2008). Mounting evidence illustrates that common neural systems are activated both in encoding and retrieval (Nyberg, Habib, McIntosh, &amp; Tulving, 2000; Wheeler, Petersen, &amp; Buckner, 2000). Crucially, the similarity between neural patterns during encoding and retrieval is often predictive of how well an experience is remembered subsequently (see Brewer, Zhao, Glover, &amp; Gabrieli, 1998; Wagner et al., 1998 for reviews). There is much evidence indicating that reinstated neural activations are specific to perceptual modality (visual vs. auditory), domain (memory for what - where) and feature (colour, motion or spatial location) (see Slotnick, 2004 for a review). For example, Wheeler, Petersen and Buckner (2000) gave participants a set of picture and sound items to study and then a recall test during which participants vividly remembered these items. Results demonstrated that regions of auditory and visual cortex are activated differently during retrieval of sounds and pictures. In a similar fashion, Goldberg, Perfetti and Schneider (2006b) asked participants whether a concrete word possesses a property from one of four sensory modalities as colour (e.g. green), sound (e.g., loud), touch (e.g., soft) or taste (e.g., sweet). Retrieval from semantic memory involving flavour knowledge as in the word sweet increased specific activation in the left orbitofrontal cortex which is known to process semantic comparisons among edible items (Goldberg, Perfetti, &amp; Schneider, 2006a). A number of studies supported a simulation account of memory with retrieval dependent on perceptions by showing temporal overlaps between encoding and retrieval (Kent &amp; Lamberts, 2008). There is not a strict temporal regularity between retrieval and encoding as far as the ERP evidence shows (Allan, Robb, &amp; Rugg, 2000). However, better memory performance was found in serial recall when retrieval direction (forward vs. backward) matched with the order in which the words were encoded in the first place (J. G. Thomas, Milner, &amp; Hanerlandt, 2003). More direct evidence for temporal similarity between encoding and retrieval comes from Kent and Lamberts (2006). Participants were instructed to retrieve different dimensions of faces such as eye colour, nose shape, mouth expressions etc. Results revealed that features that were quickly perceived were also quickly retrieved. In addition to the findings from the abovementioned research areas, the historical phenomena of state-dependent memory and context-dependent memory show that memory retrieval is simulation of the original event. An overlap between the internal state (e.g., mood, state of consciousness) or external context of the individual during encoding and retrieval leads to higher retrieval efficiency (S. M. Smith &amp; Vela, 2001; Ucros, 1989). In one such study, Dijkstra, Kaschak and Zwaan (2007) documented faster retrieval when body positions and actions during retrieval of autobiographical events were similar to the body positions and actions in the original events compared to when body positions and actions were non-congruent. For example, participants were faster to remember how old they were at a concert, if they were instructed to sit up straight in the chair and clap their hands several times during the retrieval. In another intriguing study (Casasanto &amp; Dijkstra, 2010), participants were instructed to tell their autobiographical memories with either positive or negative valence, while moving marbles either upward or downward, which was an apparently meaningless action. However, retrieval was faster when the direction of movement was congruent with the valence of the emotional memory in a metaphorical way (i.e., upward for positive and downward for negative memories). Lastly, eye movements provide plentiful evidence that retrieval is perceptual recreation of encoding (D. C. Richardson &amp; Spivey, 2000; Spivey &amp; Geng, 2000) and further, these simulations usually predict the success of the retrieval (Johansson &amp; Johansson, 2014; Scholz et al., 2018, 2016). Eye movements in memory simulations are further elaborated in Chapter 2.2.3. 2.1.3 Simulations in language Language is one of the most influential domains in showing the centrality of simulations in human cognition. The claim of simulation view of language is simple: Meaning centrally involves the activation of perceptual, motor, social, and affective knowledge that characterizes the content of utterances (Bergen, 2007, pp. 277-278). Thus, a simulation mechanism is essential to comprehend and remember language. Switch-cost effects are a clear demonstration of perceptual and affective (re)activation in language. In this paradigm, participants are asked to verify whether a property (e.g., blender) corresponds to a particular target modality (e.g., loud in the auditory modality). The effect is that participants are slower to verify a property in one perceptual modality (e.g., blender can be loud - auditory modality) after verifying a property in a different modality (e.g., cranberries can be tart - gustatory modality) than after verifying a property in the same modality (e.g., leaves can rustle - auditory modality) (Pecher, Zeelenberg, &amp; Barsalou, 2003). A switch-cost occurs between properties with positive and negative valence (e.g., couple can be happy, and orphan can be hopeless) (Vermeulen, Niedenthal, &amp; Luminet, 2007) and at the sentence level (e.g., A cellar is dark in visual modality - A mitten is soft in tactile modality) (Hald, Marshall, Janssen, &amp; Garnham, 2011). Similar switching costs occur when participants switch between actual modalities in perceptual tasks (Masson, 2015). Thus, findings reviewed above support the claim that language is rooted in perceptions and language comprehension can activate these perceptions. Importantly, the same priming effect was not elicited when participants verified semantically associated properties (e.g., sheet can be spotless, and air can be clean) as opposed to unassociated properties (e.g., sheet can be spotless, and meal can be cheap) (Pecher et al., 2003). This finding rules out the alternative, computational hypothesis that properties across all modalities are stored together in a single, amodal system of knowledge. Rather, they support perceptual roots of language processing and language-based simulations. 2.1.3.1 Mental simulations and situation models Simulations triggered with language are slightly different than the sensorimotor simulations that have been covered so far. Sensorimotor simulations in mental imagery and memory rely on actual sensorimotor experiences (e.g., playing a piano or perceptually encoding an episode). They take place in an offline manner, that is, when the agent needs to access perceptual/conceptual information in the absence of original stimulus. Whereas, language-based simulations are activated upon perceiving linguistic stimuli in an online manner. The subject (re)creates perceptual, motor, affective, introspective and bodily states not by actually experiencing them but through linguistic descriptions. Further, language can give rise to simulations of several abstract conceptualisations that go beyond these states. This type of simulation is usually referred to as a mental simulation (Zwaan, 1999). Mental simulations can extend into and affect subsequent perceptual/conceptual processing and memory retrieval (discussed below). It is reasonable to assume that online mental simulation evoked by language and offline simulation in memory and mental imagery share some common architecture. After all, both types of simulations originate from perceptual, motor, affective, introspective and bodily states. That said, the substantial difference between offline and online simulation is conscious effort. Mental simulations based on language are assumed to be inherently involved in language comprehension and thus, triggered automatically and unconsciously (Zwaan &amp; Pecher, 2012). Whereas, offline sensorimotor simulation in memory and mental imagery is often a consequence of effortful, resource-consuming and conscious processes as memory and mental imagery themselves. In line, there is little to no evidence that mental simulation is correlated with the strength of mental imagery (Zwaan &amp; Pecher, 2012). The idea of mental simulation via language stems from the discovery of mirror neurons (Caggiano et al., 1996; Gallese et al., 1996). Mirror neurons are activated in motor regions of the brain by merely observing others executing motor actions (Hari et al., 1998). In a similar fashion, neural correlates were found between the content of what is being read and activated areas in the brain (see Hauk &amp; Tschentscher, 2013; Binkofski, 2010; Pulvermüller, 2005 for exhaustive reviews and Jirak, Menz, Buccino, Borghi for a meta-analysis). In a pioneering study (Hauk, Johnsrude, &amp; Pulvermüller, 2004), participants saw action words referring to face, arm and leg (e.g., lick, pick and kick) in a passive reading task and then, moved their corresponding extremities (i.e., left or right foot, left or right index finger, or tongue). Results showed that reading action verbs activates somatotopic brain regions (i.e., regions corresponding to specific parts of the body) that are involved in the actual movements (see also Buccino et al., 2005). For example, reading the word kick or pick invokes activation in the specific regions of motor and premotor cortex that control the execution of leg and arm movements respectively. Critically, several fMRI (functional magnetic resonance imaging) studies showed that not only concrete words but also idiomatic expressions involving action words (e.g., John grasped the idea or Pablo kicked the habit) (see Yang &amp; Shu, 2016 for a review) and counterfactual statements (e.g., if Mary had cleaned the room, she would have moved the sofa) (de Vega et al., 2014) elicit similar somatotopic activation in brain. In addition to action words, words in different perceptual modalities activate brain regions associated with the concerned modalities as well. For example, reading odour-related words such as cinnamon, garlic or jasmine triggers activations in primary olfactory cortex, the brain region involved in the sensation of smells (González et al., 2006). Language-based simulations go beyond recreation of perceptual and motor experiences. It is well-documented that reading narratives can form situation models (mental models) in the minds of the readers (e.g., Speer, Reynolds, Swallow, &amp; Zacks, 2009). Situation models are integrated, situational mental representations of characters, objects and events that are described in narrative (JohnsonLaird, 1983; Kintsch &amp; van Dijk, 1978). They allow readers to imagine themselves in the story by taking the perspective of the protagonist (e.g., Avraamides, 2003). Consequently, situation models give rise to simulations of perceptual, motor and affective states and also abstract structures such as time, speed, space, goals and causations (Speed &amp; Vigliocco, 2016; Zwaan, 1999; Zwaan &amp; Radvansky, 1998). For instance, Zwaan, Stanfield and Yaxley (2002) evidenced that language comprehenders simulate what the objects described by language look like. In their study, participants read sentences describing an animal or an object in a certain location (e.g., egg in a carton vs. egg in a pan). Thus, the shape of the objects changed as a function of their location, but it is only implied by sentences (e.g., The egg is in the carton. - whole egg). Even though, a line drawing of the object matching with the shape implied in the previous sentence (e.g., a drawing of a whole egg) improved participants performance in retrieval of the sentences. Similar results were demonstrated for sentences that imply orientation (e.g., vertical - horizontal) (D. C. Richardson, Spivey, Barsalou, &amp; McRae, 2003), rotation (Wassenburg &amp; Zwaan, 2010), size (de Koning, Wassenburg, Bos, &amp; Van der Schoot, 2017), colour (Zwaan &amp; Pecher, 2012), visibility (Yaxley &amp; Zwaan, 2007), distance (Vukovic &amp; Williams, 2014) and number (Patson, George, &amp; Warren, 2014). Language can activate simulations of more abstract structures in the same manner. Simulation of time, in particular, is well-documented. For instance, longer chronological distance between two consecutively narrated story events denoted with an hour later as compared to a moment later leads to longer reading times (Zwaan, 1996). Reading times measured with eye movements were also shown to be longer when reading slow verbs (e.g., amble) than fast verbs (e.g., dash) (Speed &amp; Vigliocco, 2014). Similarly, Coll-Florit and Gennari (2011) found that judging the sensicality of sentences describing durative states (e.g. to admire a famous writer) took longer than non-durative states (e.g. to run into a famous writer). Several other abstractions can be mentally simulated in the readers mind. In one experiment, participants can access the concept of cake more easily when they previously read a sentence in which a cake is actually present (Mary baked cookies and cake) than when it is not (Mary baked cookies but no cake) (MacDonald &amp; Just, 1989). In another experiment, participants simulated the protagonists thoughts and they remembered and forgot what the character in the story remembered and forgot (Gunraj, Upadhyay, Houghton, Westerman, &amp; Klin, 2017). In Scherer, Banse, Wallbott and Goldbeck (1991), participants simulated the intended emotions that were cued in characters voices. Mental simulations via language, and situation models play important roles in numerous cognitive tasks transcending language comprehension. Most importantly, simulations are involved in memory for language. Johansson, Oren and Holmqvist (2018) reported that eye movements on a blank screen when participants were remembering a narrative reflected the layout of the scenes described in the text rather than the layout of the text itself. Zwaan and Radvansky (1998) assumed that successful retrieval of what is comprehended would necessarily involve the retrieval of simulations. In accordance with this assumption, there is evidence that the ability to restructure situation models have beneficial effects on memory performance (Garnham, 1981; Magliano, Radvansky, &amp; Copeland, 2012). 2.1.3.2 Simulation of space with language Space has a privileged status in human cognition. Coslett (1999) argues that the representation of space in the mind has a fundamental evolutionary advantage because information about the location of objects in the environment is essential for sustenance and avoiding danger. A large body of evidence indicates that young children show sensitivity to spatial concepts and properties starting from the infancy (e.g., Aguiar &amp; Baillargeon, 2002; Casasola, 2008; Frick &amp; Möhring, 2013; Hespos &amp; Rochat, 1997; McKenzie, Slater, Tremellen, &amp; McAlpin, 1993; Örnkloo &amp; Von Hofsten, 2007; Wishart &amp; Bower, 1982). There is also evidence suggesting that development of spatial cognition forms the foundation for subsequent cognitive structures such as mathematical aptitude (Lauer &amp; Lourenco, 2016), creativity (Kell, Lubinski, Benbow, &amp; Steiger, 2013) and notably, language (Levinson, 1992; Piaget &amp; Inhelder, 1969). As a result, there is good reason to assume that language and space are inherently interconnected through the course of cognitive development (e.g., Casasola, 2005; Haun, Rapold, Janzen, &amp; Levinson, 2011; Hespos &amp; Spelke, 2004). People use language when describing space and spatial language schematises space by selecting certain aspects of a scene while ignoring other aspects (Talmy, 1983). For instance, across conveys the information that the thing doing the crossing is smaller than the thing that is being crossed (Tversky &amp; Lee, 1998). However, it does not contain any information about the distance between these things or their shapes. Thereby, language forms spatial representations in mind (H. A. Taylor &amp; Tversky, 1992). On the other hand, space provides a rich canvas for representing abstraction. Many abstract conceptualisations such as time (Boroditsky &amp; Ramscar, 2002), valence (Meyer &amp; Robinson, 2004), power (Zanolie et al., 2012), numerical magnitude (Dehaene et al., 1993), happiness (Damjanovic &amp; Santiago, 2016), divinity (Chasteen, Burdzy, &amp; Pratt, 2010), health (Leitan, Williams, &amp; Murray, 2015) and self-esteem (J. E. T. Taylor, Lam, Chasteen, &amp; Pratt, 2015) are understood with space (e.g., powerful is up, more is up, happy is up etc.). Further, space constraints the use of language with gestures and in sign language (Emmorey, 2001; Emmorey, Tversky, &amp; Taylor, 2000). In support of this, both brain imaging (Carpenter, Just, Keller, Eddy, &amp; Thulborn, 1999) and behavioural (Hayward &amp; Tarr, 1995) evidence indicate that there are similarities between spatial and linguistic representations. Given the central position of space in human mind as briefly discussed above and the intrinsic links between language and space, spatial simulations in language deserve particular attention. Reading narratives can activate simulations of spatial descriptions in a text through situation models. For instance, objects that are described close to a protagonist in a narrative are accessed faster than the objects described as more distant (Glenberg, Meyer, &amp; Lindem, 1987; Morrow, Greenspan, &amp; Bower, 1987). In seminal work, Franklin and Tversky (1990) showed that situation models of space derived from text are similar to the representations of spatial experiences in the real-world and notably, have bodily constraints. Participants in the study read descriptions of scenes and objects in them. Then, they were asked to remember and locate certain objects in a three-dimensional environment. Results showed that objects on the vertical (i.e., head-feet) axis were retrieved faster than objects on the horizontal (i.e., left-right) and sagittal (i.e., front-back) axes. The findings indicate that space in language is simulated with an ego-centric perspective rather than an allocentric (i.e., object-centred) or a mental transformation perspective. If the participants took an allocentric perspective as in inspecting a picture (in which the subject is not immersed into the environment), all directions would have been equally accessible. On the other hand, if they mentally transformed the described environments, response times would have varied as a function of the mental movement needed to inspect each location. Accordingly, response times would have been shortest for the objects in front of the subject and the accessibility would have decreased in line with the angular disparity from the front. Objects behind the subject, for example, would have been the most difficult to access. Bias for the objects on the vertical dimension suggests that simulation of space with language is body-based. As Franklin and Tversky (1990, p. 64) discuss, the dominant position of a person interacting with the environment is upright due to a number of reasons: First, the perceptual world of the observer can be described by one vertical and two horizontal dimensions (i.e., left/right and front/back). Second, vertical dimension is correlated with gravity, which in an important asymmetric factor in perceiving spatial relations. Thus, vertical spatial relations generally remain constant with respect to the observer. Third, the ground and the sky present stationary reference points on the vertical axis. On the other hand, horizontal spatial relations change frequently. Thus, horizontal dimension depends on more arbitrary reference points, such as the prominent dimensions of the observers own body. In another experiment using a similar methodology (Avraamides, 2003), it was demonstrated that simulated ego-centric positions are not static but can be automatically updated whenever the reader/protagonist moves in the text, suggesting the motor basis of language. In a recognition memory task, Levine and Klin (2001) showed that a story characters current location was more active in the readers memory than her/his previous location (see Gunraj et al., 2017). Further, such spatial simulations remained highly accessible even several sentences after last mention, indicating the robustness of these spatial simulations. There are stable representational mappings between language and space at the sentence level as well. Richardson, Spivey, Edelman and Naples (2001) asked participants to read sentences involving concrete and abstract action verbs (e.g., lifted, offended). They were then asked to associate diagrams illustrating motions on the horizontal (left and right) and the vertical axis (up and down) with the sentences depicting motion events. Substantial agreement was found between participants in their preferences of diagrams for both concrete and abstract verbs within action sentences. For example, participants tended to attach a horizontal image schema to push, and a vertical image schema to respect. In a later study, it was evidenced that spatial simulation triggered by a verb affects other forms of spatial processing along the same axis both in a visual discrimination and a picture memory task (D. C. Richardson et al., 2003). Spatial simulations interfered with visual discrimination on the congruent axis and deteriorated performance; however, memory performance was facilitated when the picture to be remembered and the simulated orientation matched (see Effects of mental simulation below). The effect was shown for both concrete and abstract verbs. Not only orientation, but upward and downward motion on the vertical axis are simulated via language. In one study (Bergen, Lindsay, Matlock, &amp; Narayanan, 2007), subject nouns and main verbs related with up and down locations interfered with visual processing in the same location. However, the effect was shown in literal sentences implying real space (e.g., The ceiling cracked  downward movement for the subject noun, The mule climbed  upward movements for the main verb) but not in sentences implying metaphorical space (e.g., The prices rose). Bergen et al. (2007) argue that the comprehension of the sentence as a whole, and not simply lexical associations, yield spatial simulations. However, there is evidence that single words can also trigger simulation of space. Several abstract nouns such as tyrant (up) and slave (down) invoke simulations of metaphorical spatial locations (e.g., Giessner &amp; Schubert, 2007). There are numerous common nouns in language such as bird (up) and worm (down) which are associated with actual spatial locations (i.e., spatial iconicity). Words denoting spatial locations simulate perceptions of these locations in space. In Zwaan and Yaxley (2003), participants were presented word pairs with spatial associations (e.g., attic - basement) and asked to decide whether the words are semantically related. Results showed that word pairs in a reverse-iconic condition (i.e., basement above attic) were judged slower than word pairs in an iconic condition (i.e., attic above basement). In a similar fashion, it was shown that reading words that occur higher or lower positions in the visual field (e.g., head and foot) hinders the identification of visual targets at the top or bottom of the display (Estes, Verges, &amp; Barsalou, 2008). 2.1.3.3 Effects of mental simulations Simulation-based language understanding leads to two main effects on simultaneous or subsequent visual/conceptual processing: compatibility and interference (see Fischer &amp; Zwaan, 2008 for a review). The underlying idea is that if understanding an utterance involves the activation of perceptual, affective and motor representations; then perceptions, emotions and actions that are congruent with the content of the utterances should facilitate visual/conceptual processing and vice versa (Bergen, 2007). For example, the action-sentence compatibility effect demonstrates compatibility/interference resulting from motor simulations in language. In the study introducing the effect for the first time (Glenberg &amp; Kaschak, 2002), participants were presented sensible and non-sensible sentences (e.g., Boil the air) and were asked to judge whether the sentences made sense or not. Sensible sentences implied actions either toward the body (e.g., Open the drawer) or away from the body (e.g., Close the drawer). Response button for identifying the sentence as sensible (i.e., yes button) was either near or far from the participants bodies. Results showed that when the implied direction of the sentence and the actual action to press the button matched, participants were faster to judge the sensibility of the sentences. For example, the sentence, Open the drawer was processed faster when participants reached the yes button near them, an action that is comparable to opening a drawer. The effect was found not only for imperatives but also for descriptive sentences (Andy delivered the pizza to you - toward sentence / You delivered the pizza to Andy - away sentence). Notably, sentences describing abstract transfers (Liz told you the story - toward sentence / You told Liz the story - away sentence) elicited an action-sentence compatibility effect as well. An action-sentence compatibility effect extends to sign language, suggesting that the motor system is involved in the comprehension of a visual-manual language as well (Secora &amp; Emmorey, 2015). Notably, the congruency effect was found relative to the verbs semantics (e.g., You throw a ball - away) not relative to the actual motion executed by the signer and perceived by the participant (e.g., You throw a ball - toward). Along with that, there are meta-reviews and experimental evidence arguing that an action-sentence compatibility effect is generally weak (Papesh, 2015; but see Zwaan, van der Stoep, Guadalupe, &amp; Bouwmeester, 2012) or highly task-dependent (Borreggine &amp; Kaschak, 2006). In sum, the current status of the literature suggests that the factors modulating an action-sentence compatibility effect and in general, effects of language-based simulations should be further specified. Simulations can also interfere with language comprehension which results in a mismatch advantage. For example, Kaschak et al. (2005) demonstrated that participants judge the feasibility of motion sentences (e.g., The horse ran away from you) faster when they simultaneously view visual displays depicting motion in the opposite direction as the action described in the sentence (e.g., a spiral moving towards the centre). They concluded that visual processing and action simulation during language comprehension engage the same neural circuits; which, in turn leads to a mismatch advantage. Connell (2007) evidenced a mismatch advantage in the simulation of colour with language. Participants read sentences involving an object which can occur in different colours (e.g., meat can be red when raw and brown when cooked). They were then presented pictures of objects and they had to decide whether the pictured object had appeared in the preceding sentence. Colour of the objects sometimes matched with the descriptions in the sentences (e.g., John looked at a steak in the butchers window - red steak) and sometimes did not match (e.g., John looked at a steak in the butchers window - brown steak). Responses were faster when the colour of the object mismatched with the colour implied by the previous sentence. Why do some studies show a congruency advantage and others an incongruency advantage? This is an important question within the context of the present thesis (see Chapter 6). Kaschak et al. (2005) argue that there are two factors determining match or mismatch advantage in language-based simulations: (1) Temporal distance between the perceptual stimulus and the verbal stimulus to be processed. (2) The extent to which the perceptual stimulus can be integrated into the simulation activated by the content of the sentence. In support of the temporal distance assumption, Borreggine and Kaschak (2006) found that action-sentence compatibility effect arises only when individuals have enough time to plan their motor response as they process the sentence. According to Kaschak et al. (2005), if the verbal information must be processed simultaneously with the perceptual information, a congruency or incongruency advantage may occur, depending on whether linguistic information and perceptual stimulus can be integrated. To be more specific, a congruency advantage is expected if the linguistic and visual stimulus are comparable such as reading the sentence The egg is in the carton and seeing a line drawing of a whole egg (Zwaan et al., 2002). However, different perceptual and linguistic stimulus such as reading the sentence The horse ran away from you and seeing a spiral moving towards the centre or away from it (Kaschak et al., 2005) result in an incongruency advantage (see also Meteyard, Zokaei, Bahrami, &amp; Vigliocco, 2008) 2.2 I Look, Therefore I Remember: Eye Movements and Memory 2.2.1 Eye movements and eye tracking Eyes do not flow in a smooth fashion when engaged in visual tasks (Huey, 1908). If you were able to see your gaze on the page or on the digital screen right now, you would notice that your eyes shift from one word to the next as you are reading this sentence. Known as saccades, these jumps are rapid, short and repeated ballistic (i.e. jerk-like) movements which occur approximately three to four times every second. Saccades abruptly change the point of fixations, the periods of eye immobility in which visual or semantic information is acquired and processed (Purves, Augustine, &amp; Fitzpatrick, 2001; D. C. Richardson &amp; Spivey, 2004). In simple terms, individuals internalise the visual world during fixations that are executed between saccades (Bridgeman, Van der Heijden, &amp; Velichkovsky, 1994; Simons &amp; Rensink, 2005). Eye movements are fundamental to visual perception because visual system cannot process the huge amount of available information in the visual world at once. Thus, execution of eye movements allows us to see the world as a seamless whole, although we can only see one region at a time (Buswell, 1936; Yarbus, 1967) due to anatomical limitations (i.e., the total visual field that the human eye covers) and also, limited processing resources (Levi, Klein, &amp; Aitsebaomo, 1985; D. C. Richardson, Dale, &amp; Spivey, 2007). Fixations have two elemental measures: location and duration. Both measures are highly informative of ongoing cognitive operations. We can see a stimulus clearly only when it falls into the most sensitive area of the retina (i.e., fovea) (~2o or 3 to 6 letter spaces), which is specialised for high acuity visual perception (Mast &amp; Kosslyn, 2002; Yarbus, 1967). Thus, eye position (i.e., fixation location) gives valuable information about the location of the attentional spotlight (Posner, Snyder, &amp; Davidson, 1980). In other words, fixation location corresponds to the spatial locus of cognitive processing. On the other hand, fixation duration corresponds to the duration of cognitive processing of the material located at fixation (Irwin, 2004, p. 2). Longer fixations suggest higher cognitive load or higher attentional processing demands required by a material or task (Irwin, 2004). The underlying idea behind the link between cognition and fixation is known as eye-mind assumption (Just &amp; Carpenter, 1980), which simply posits that the direction of our eyes indicates the content of the mind (Underwood &amp; Everatt, 1992). Based on the location and duration of fixations, cognitive processes can be measured and evaluated objectively and precisely during the occurrence of the process in question. There is now a universal consensus on the value of eye movements and eye tracking as a methodology in the investigation of the human mind (e.g., Hyona, Radach, &amp; Deubel, 2003; Just &amp; Carpenter, 1980; Rayner, 1998; Rayner, Pollatsek, Ashby, &amp; Clifton, 2012; Reichle, Pollatsek, Fisher, &amp; Rayner, 1998; Theeuwes, Belopolsky, &amp; Olivers, 2009; Van der Stigchel et al., 2006). Eye tracking methodology provides detailed measures with regard to the temporal order of fixations and saccades, gaze direction, pupil size and time spent on pre-defined regions of the scene. Fixation duration in a certain location relative to other locations is used as the main measure of looking behaviour in the present thesis. Eye movements can be monitored in various different ways. A pupil corneal reflection technique, that is based on high-speed cameras and near infrared light, is the most advanced remote and non-intrusive eye tracking method as of today. An illuminator shines dispersed infrared light to one eye or both eyes. A high-speed video camera captures the infrared reflections coming from the pupil and cornea (i.e., the outer layer of the eye) and transforms them into high-resolution images and patterns pertaining to the position of the eye(s) at any given millisecond. Such an infrared eye tracker can record eye movements quite precisely. Precision offered by an eye tracker is indicated by temporal resolution (i.e., sampling rate) and spatial resolution. Sampling rate shows the frequency of which a tracker samples and determines the position of the eye at a given moment. For example, the eye tracker used in the present thesis (i.e., SR EyeLink 1000) operates at a sampling rate of 1000 Hz, which means that the position of the eye is measured 1000 times every second. Put differently, it produces one sample of the eye position per one millisecond. Spatial resolution refers to the angular distance between successive samples of eye position. Thus, an eye tracker with a higher spatial resolution can detect even the smallest eye movements in a certain interest area. SR EyeLink 1000 has a spatial resolution of 0.25o - 0.50o which means that it can detect and sample eye movements within an angular distance of 0.25o - 0.50o. There generally exists a spatial difference between the calculated location of a fixation and the actual one. This difference is expressed in degrees of visual angle and reflects the accuracy of eye tracking. If you draw a straight line from the eye to the actual fixation point on the screen and another line to the computed one, the angle between these lines gives the accuracy. Thus, a smaller difference means higher accuracy. Accuracy depends on the screen size and the distance between the participant and the screen. Visual angle is also used to calculate the size of the experimental stimulus as it refers to the perceived size rather than the actual size. These measures of data quality are reported in the methods section of each experiment in accordance with the eye tracking standards and good practices in literature (Blignaut &amp; Wium, 2014; Holmqvist, Nyström, &amp; Mulvey, 2012; D. C. Richardson &amp; Spivey, 2004). 2.2.2 Investigating memory with eye movements The role of eye movements in evidently visual tasks and processes such as visual perception (Noton &amp; Stark, 1971), reading (Rayner, 1998), visuospatial memory (Irwin &amp; Zelinsky, 2002), visual search (Rayner, 2009) and visuospatial attention (Van der Stigchel et al., 2006) has been widely investigated for many decades and is very well-documented. Eye movements have recently emerged as an alternative means in memory research complementing behavioural measures based on end-state measures (e.g., hit rate, hit latency etc.) (Lockhart, 2000) and brain-imaging studies (Fiser et al., 2016; Gabrieli, 1998; Rugg &amp; Yonelinas, 2003). It has been known for a long time that previous experience and knowledge of the observer can govern eye movements in addition to the physical properties of the scene and stimulus. For example, many early studies have reported that human observers tend to look at areas of a picture which are relatively more informative to them. Importantly, informativeness rating of a region is modulated by the previous knowledge of the participants in the long-term memory (Antes, 1974; Kaufman &amp; Richards, 1969; Mackworth &amp; Morandi, 1967; Parker, 1978; Zusne &amp; Michels, 1964). Similarly, Althoff and Cohen (1999) reported that previous exposure to a face changes the viewing behaviour and thus, eye movements. In their study, different patterns of eye movements emerged when participants viewed famous versus non-famous faces driven by recognition, fame rating and emotion labelling tasks. Participants made fewer fixations and fixation durations were shorter when viewing famous faces (now known as a repetition effect), which suggests lower cognitive load in processing previously experienced stimuli that can be retrieved from memory. Ryan, Althoff, Whitlow and Cohen (2000) took a similar approach: Participants viewed a set of real word images under three conditions: novel (i.e., seen once during the experiment), repeated (i.e., seen once in each block of the experiment) or manipulated (i.e., seen once in original form in the first two blocks and then seen in a slightly changed form in the final block). Participants made fewer fixations and sampled fewer regions when viewing repeated and manipulated scenes compared to novel scenes (i.e., repetition effect). Repetition effect speaks to the link between stability of mental representation and memory-guided eye movements. To illustrate, in Heisz and Shore (2008), the number of fixations gradually decreased with the number of exposures to the unfamiliar faces during a task. There was also evidence for another memory driven eye movement behaviour known as a relational manipulation effect: a higher proportion of total fixation time was dedicated to the manipulated regions in the scenes compared with repeated or novel scenes. Further, participants made more transitions into and out of the changed regions of the manipulated scenes than in unchanged (matched) regions of the repeated scenes. Similar paradigms based on eye movements were also used to study memory in non-human primates (Sobotka, Nowicka, &amp; Ringo, 1997), infants (Richmond, Zhao, &amp; Burns, 2015; Richmond &amp; Nelson, 2009) and special populations. For example, Ryan et al. (2000, Experiment 4) did not observe any difference in looking patterns between amnesic patients with severe memory deficits and a control group when both were viewing the repeated images. However, amnesic patients did not look longer at the altered regions when viewing manipulated images, suggesting that amnesia disrupts relational memory, i.e., memory for the relations among the constituent elements of an experience. Likewise, in Niendam, Carter and Ragland (2010), schizophrenic patients failed to detect image manipulation, which was shown with eye movements and even though the memory impairment was not evident in behavioural results. Studies reviewed above suggest relevance of eye movements in memory and importantly, advantages of eye tracking methodology over behavioural, response-based methodologies. (1) Memory-guided eye movements are mostly obligatory, that is, cannot be controlled. For instance, repetition effect reviewed above occurs regardless of the instruction (i.e., whether participants are told just to study all items for later, are explicitly told to pick out the familiar item, or are told to avoid looking at the familiar item) (Ryan, Hannula, &amp; Cohen, 2007, pp. 522-523). (2) Individuals launch memory-guided eye movements whether exposure comes from short term memory (i.e., within the experiment) or from long term memory (i.e., prior to the experiment). (3) Memory-guided eye movements precedes conscious recall. As stated by Hannula et al. (2010), eye movements can reveal memory for elements of previous experience without appealing to verbal reports and without requiring conscious recollection (see Spering &amp; Carrasco, 2015 for a comprehensive review; but see Smith, Hopkins, &amp; Squire, 2006). For instance, repetition effect occurs as early as the very first fixation to the item and thus, prior to the behavioural recognition response (Ryan et al., 2007). Similarly, in Henderson and Hollingworth (2003), gaze durations were reliably longer for manipulated scenes although participants failed to detect changes explicitly. To conclude, studies making use of eye movements are highly promising as a methodology. They can provide unique information about memory processes, which complement overt behavioural measures and brain imaging (e.g., Hannula &amp; Ranganath, 2009). In fact, eye movements are so representative of memory that mathematical models are able to predict the task that a person is engaged in (e.g. scene memorisation) from their eye movements using pattern classification (Henderson, Shinkareva, Wang, Luke, &amp; Olejarczyk, 2013). It should also be noted that eye movements in memory are not limited to fixation measures or saccadic trajectories. Variation in pupil size (e.g., pupil dilation) and blinks have been used to probe the ongoing processes during retrieval (Goldinger &amp; Papesh, 2012; Heaver &amp; Hutton, 2011; Kahneman &amp; Beatty, 1966; Mill, OConnor, &amp; Dobbins, 2016; Otero, Weekes, &amp; Hutton, 2011; Siegle, Ichikawa, &amp; Steinhauer, 2008; Van Gerven, Paas, Van Merriënboer, &amp; Schmidt, 2004; Vo et al., 2008). A well-established finding is that the pupil dilates as the retrieval becomes cognitively challenging (Goldinger &amp; Papesh, 2012; Kucewicz et al., 2018; Laeng, Sirois, &amp; Gredeback, 2012). 2.2.3 Eye movements in mental imagery and memory simulations As discussed in Chapter 2.1.1 and 2.1.2, there is mounting evidence showing the neural and behavioural similarities between memory and mental imagery (Albers et al., 2013; Rebecca Keogh &amp; Pearson, 2011). Concordantly, simulation theories of memory within grounded-embodied cognition highlight the connection between memory and mental imagery in that both processes are simulations in essence. That is, memory retrieval/mental imagery is a neural, perceptual and/or motor reinstatement of perception (Borst &amp; Kosslyn, 2008; Buckner &amp; Wheeler, 2001; De Brigard, 2014; Ganis et al., 2004; Kent &amp; Lamberts, 2008; Mahr &amp; Csibra, 2018; Michaelian, 2016b; Norman &amp; OReilly, 2003; Pasternak &amp; Greenlee, 2005; Shanton &amp; Goldman, 2010). Eye movements play a crucial role in the simulation thesis of memory and mental imagery because they can illustrate the behavioural reinstatements between perception/encoding and imagery/retrieval. The essential idea behind this imagery - eye movements - memory network holds that eye movements are stored in memory along with the visual representations of previously inspected images and they are re-enacted during memory and visual imagery (Mast &amp; Kosslyn, 2002). Long before the idea had been proven empirically, many researchers hinted at a possible similarity in saccades between visual perception and imagery (Hebb, 1968; Hochberg, 1968; Neisser, 1967; Schulman, 1983). Hebb (1968) was probably the first researcher who explicitly argued that if the mental image is a reinstatement of the perceptual process, it should include the eye movements (p. 470). Brandt and Stark (1997) provided direct empirical evidence for this argument by showing that people do move their eyes during mental imagery and the scanpaths (i.e., the sequential order of fixations and saccades, not only their spatial positions) are not random (see also Noton &amp; Stark, 1971 for more on scanpath theory). Instead, they bear striking similarities with the scanpaths during the perception of the original image (Foulsham &amp; Underwood, 2008; Underwood, Foulsham, &amp; Humphrey, 2009) Correspondence between the eye movements in perception and imagery was so robust that it was observed both for auditory (retelling a story) and visual stimuli (depicting a picture) and even when participants were in complete darkness and thus, without any visual information at all during imagery (Johansson et al., 2006). It seems reasonable to assume that spatiotemporal characteristics of visual perception are similar to the mental imagery as eye movements reflect the mental processes during visual inspection. Memory-guided eye movements are also informative in grounding of abstract concepts such as time. In Martarelli, Mast and Hartmann (2017), participants launched more rightward saccades during encoding, free recall and recognition of future items compared to past items (see also Hartmann, Martarelli, Mast, &amp; Stocker, 2014; Stocker, Hartmann, Martarelli, &amp; Mast, 2015). A majority of the studies investigating the ocular motility in mental imagery and memory have revolved around the role and functionality of eye movements. Whether these eye movements are merely epiphenomenal (i.e., an involuntary by-product of the imagery process) or play an important role and affect the imagery/retrieval performance is an important issue in that it directly taps into the primary question of nonvisual gaze patterns: Why do people move their eyes when forming mental images in the first place? Early studies (Kosslyn, 1980) discussed a potential advantage in vividness if non-random eye movements are systematically employed during mental imagery; yet, they failed to provide experimental evidence, which led to a premature conclusion: Oculomotor movements during imagery were regarded as a mere reflection of the visual buffer (Kosslyn, 1980, 1987). A visual buffer is a hypothetical unit which is responsible for holding visual information for a limited time. Nonvisual eye movements in mental imagery were assumed as an additional mechanism for presenting complex scenes on the visual buffer without overloading its capacity (Brandt &amp; Stark, 1997). Thus, eye movements were viewed as passively mirroring the attentional window over the target image during encoding to provide a solution for the cognitive load problem (Irvin &amp; Gordon, 1998). There is now increasing evidence that eye movements have a relatively more direct role in mental imagery and memory (Bochynska &amp; Laeng, 2015; Hollingworth &amp; Henderson, 2002; Laeng et al., 2014; Mäntylä &amp; Holm, 2006; Stark &amp; Ellis, 1981; Underwood et al., 2009; Valuch, Becker, &amp; Ansorge, 2013). For example, in Laeng and Teodorescu (2002), participants viewed an irregular checkerboard, similar to the one used by Brandt and Stark (1997) or a coloured picture. Then, they were asked to mentally imagine the visual stimuli as they were looking at a blank screen. Percentages of fixation time on certain interest areas and the order of scanning during perceptual phase (i.e., original image) and imagery phase (i.e., blank screen) were highly correlated. But importantly, the strength of relatedness between scanpaths predicted the vividness of mental imagery. More recent evidence indicates that what is perceptually simulated in memory retrieval or mental imagery is not the order of eye movements (i.e., scanpaths) but rather, the locations of perception. In a visual memory experiment, Johansson, Holsanova, Dewhurst and Holmqvist (2012) found no literal re-enactment during retrieval although suppression of eye movements hindered retrieval accuracy (cf., Bochynska &amp; Laeng, 2015). By challenging the scanpath theory, they deduced that eye movements during retrieval are functional but not one-to-one reactivation of the oculomotor activity produced during perception/encoding (see also Foulsham &amp; Kingstone, 2012 for similar results). Also, in Laeng and Teodorescu (2002), the participants who were not allowed to free scan during imagery phase (i.e., fixed gaze condition) did worse when they were asked to recall the original pattern, which was calculated by the number of squares corresponded to the location of a black square in the grid. Using a similar paradigm in visuospatial memory, Johansson and Johansson (2014) asked participants to view objects distributed in four quadrants at the encoding phase. Participants then listened statements about the direction of the objects (e.g., The car was facing left) and were asked to decide whether the statements are true or false. Results showed that participants who were free to look at a blank screen during retrieval had a superior retrieval performance than participants whose eye movements were constrained to a central fixation point. Further, participants whose eye movements were constrained to the previous locations of the objects were more accurate and faster than participants whose eye movements were constrained to a diagonal location as to the previous location of the concerned object. Studies reviewed above suggest that the human mind encodes eye movements not as they are but in the form of spatial indices, seemingly invisible spatial pointers in space (D. C. Richardson &amp; Kirkham, 2004; D. C. Richardson &amp; Spivey, 2000). Spatial indices link internal representations to objects in the visual world by tapping into space-time information and in turn, trigger eye movements to blank locations during retrieval to reduce working memory demands (Ballard et al., 1997). Therefore, there is no need for a literal recapitulation of gaze patterns because eye movements function as a scaffolding structure with the network of spatial indices for the generation of a detailed image. In other words, spatial indices in the environment which are internalised via eye movements complete the representations in the head resulting in a detailed mental image (Ferreira et al., 2008). In an alternative model, ORegan and Noë (2001) put forward that seeing is a way of acting and eye movements are visual representations themselves in a nod to ecological psychology (Gibson, 1979). To sum up, current evidence shows that oculomotor activity during memory and mental imagery is not limited to the reconstruction of the original: it is essential to generate mental images. Further, it seems that the role of eye movements is also beyond an automatic and involuntary distribution of limited cognitive sources between the oculomotor activity and memory to alleviate the mental load. Rather, eye movements might serve as an optional, situational strategy in situations where expanding could make a difference for solving the task (Hayhoe et al., 1998; Laeng et al., 2014; J. T. E. Richardson, 1979). In support of this assumption, many task-oriented vision studies have suggested that the eyes are positioned at a point that is not the most visually salient but is the best for the spatio-temporal demands of the job that needs to be done (Hayhoe &amp; Ballard, 2005, p. 189). Furthermore, there is also intriguing evidence that these strategic, opportunistic eye movements in goal-directed behaviour are guided by a dopamine-based reward system (Glimcher, 2003; Hikosaka, Takikawa, &amp; Kawagoe, 2000). Thus, eye movements during imagery and memory can be situational and adaptive according to the task demands. For example, in Laeng, Bloem, DAscenzo and Tommasi (2014) eye movements during mental imagery concentrated in the salient, information-rich parts of the original image (e.g., head region of an animal picture in the study). Here, it is important to underline that difficulty of the task seems to be the decisive factor. For instance, memory tasks requiring relatively low cognitive load would not need a detailed mental image of the original scene to be solved and thus, retrieval should be challenging in order to observe any memory advantage (Hollingworth &amp; Henderson, 2002; Laeng et al., 2014). "]]
