<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Simulating Space when Remembering Words: Role of Visuospatial Memory | Looking for Language in Space: Spatial Simulations in Memory for Language</title>
  <meta name="description" content="4 Simulating Space when Remembering Words: Role of Visuospatial Memory | Looking for Language in Space: Spatial Simulations in Memory for Language" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Simulating Space when Remembering Words: Role of Visuospatial Memory | Looking for Language in Space: Spatial Simulations in Memory for Language" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Simulating Space when Remembering Words: Role of Visuospatial Memory | Looking for Language in Space: Spatial Simulations in Memory for Language" />
  
  
  

<meta name="author" content="Alper Kumcu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="experiment-1.html"/>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i><b>0.1</b> Abstract</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#acknowledgement"><i class="fa fa-check"></i><b>0.2</b> Acknowledgement</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#statement-of-research-question"><i class="fa fa-check"></i><b>1.1</b> Statement of Research Question</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#definition-of-concepts"><i class="fa fa-check"></i><b>1.2</b> Definition of Concepts</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#spatial-indexing"><i class="fa fa-check"></i><b>1.2.1</b> Spatial indexing</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#looking-at-nothing"><i class="fa fa-check"></i><b>1.2.2</b> Looking at nothing</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#simulation"><i class="fa fa-check"></i><b>1.2.3</b> Simulation</a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction.html"><a href="introduction.html#cognitive-offloading"><i class="fa fa-check"></i><b>1.2.4</b> Cognitive offloading</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#overview-of-thesis"><i class="fa fa-check"></i><b>1.3</b> Overview of Thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="theoretical-background.html"><a href="theoretical-background.html"><i class="fa fa-check"></i><b>2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="theoretical-background.html"><a href="theoretical-background.html#mind-recreated-simulations-in-imagery-memory-and-language"><i class="fa fa-check"></i><b>2.1</b> Mind, Recreated: Simulations in Imagery, Memory, and Language</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="theoretical-background.html"><a href="theoretical-background.html#mental-imagery-as-a-simulation"><i class="fa fa-check"></i><b>2.1.1</b> Mental imagery as a simulation</a></li>
<li class="chapter" data-level="2.1.2" data-path="theoretical-background.html"><a href="theoretical-background.html#mental-retrieval-as-a-simulation"><i class="fa fa-check"></i><b>2.1.2</b> Mental retrieval as a simulation</a></li>
<li class="chapter" data-level="2.1.3" data-path="theoretical-background.html"><a href="theoretical-background.html#simulations-in-language"><i class="fa fa-check"></i><b>2.1.3</b> Simulations in language</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="theoretical-background.html"><a href="theoretical-background.html#i-look-therefore-i-remember-eye-movements-and-memory"><i class="fa fa-check"></i><b>2.2</b> I Look, Therefore I Remember: Eye Movements and Memory</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="theoretical-background.html"><a href="theoretical-background.html#eye-movements-and-eye-tracking"><i class="fa fa-check"></i><b>2.2.1</b> Eye movements and eye tracking</a></li>
<li class="chapter" data-level="2.2.2" data-path="theoretical-background.html"><a href="theoretical-background.html#investigating-memory-with-eye-movements"><i class="fa fa-check"></i><b>2.2.2</b> Investigating memory with eye movements</a></li>
<li class="chapter" data-level="2.2.3" data-path="theoretical-background.html"><a href="theoretical-background.html#eye-movements-in-mental-imagery-and-memory-simulations"><i class="fa fa-check"></i><b>2.2.3</b> Eye movements in mental imagery and memory simulations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="experiment-1.html"><a href="experiment-1.html"><i class="fa fa-check"></i><b>3</b> Experiment 1</a></li>
<li class="chapter" data-level="4" data-path="simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><i class="fa fa-check"></i><b>4</b> Simulating Space when Remembering Words: Role of Visuospatial Memory</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="simulating-space-when-remembering-words-role-of-visuospatial-memory.html#motivation-and-aims"><i class="fa fa-check"></i><b>4.1</b> Motivation and Aims</a></li>
<li class="chapter" data-level="4.2" data-path="simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="simulating-space-when-remembering-words-role-of-visuospatial-memory.html#abstract-1"><i class="fa fa-check"></i><b>4.2</b> Abstract</a></li>
<li class="chapter" data-level="4.3" data-path="simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="simulating-space-when-remembering-words-role-of-visuospatial-memory.html#introduction-1"><i class="fa fa-check"></i><b>4.3</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="simulating-space-when-remembering-words-role-of-visuospatial-memory.html#spatial-indexing-and-looking-at-nothing-automaticity"><i class="fa fa-check"></i><b>4.3.1</b> Spatial indexing and looking at nothing: automaticity</a></li>
<li class="chapter" data-level="4.3.2" data-path="simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="simulating-space-when-remembering-words-role-of-visuospatial-memory.html#spatial-indexing-and-looking-at-nothing-dynamicity"><i class="fa fa-check"></i><b>4.3.2</b> Spatial indexing and looking at nothing: dynamicity</a></li>
<li class="chapter" data-level="4.3.3" data-path="simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="simulating-space-when-remembering-words-role-of-visuospatial-memory.html#looking-at-nothing-and-visuospatial-memory"><i class="fa fa-check"></i><b>4.3.3</b> Looking at nothing and visuospatial memory</a></li>
<li class="chapter" data-level="4.3.4" data-path="simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="simulating-space-when-remembering-words-role-of-visuospatial-memory.html#role-of-eye-movements-in-memory-retrieval"><i class="fa fa-check"></i><b>4.3.4</b> Role of eye movements in memory retrieval</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Looking for Language in Space: Spatial Simulations in Memory for Language</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simulating-space-when-remembering-words-role-of-visuospatial-memory" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Simulating Space when Remembering Words: Role of Visuospatial Memory</h1>
<div id="motivation-and-aims" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Motivation and Aims</h2>
<p>Spatial simulation within grounded-embodied cognition and cognitive offloading within the extended cognition were outlined and discussed in Chapter 1 and 2. Based on this theoretical background, this chapter describes an experimental study investigating a memory-based looking behaviour (i.e., looking at nothing) which is representative of spatial simulation and cognitive offloading. In general terms, this study aims to investigate how spatial location is simulated following the visual perception of words to support the retrieval of these words.</p>
</div>
<div id="abstract-1" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Abstract</h2>
<p>People tend to look at uninformative, blank locations in space when retrieving information. This gaze behaviour, known as looking at nothing, is assumed to be driven by the use of spatial indices associated with external information. In the present study, we investigated whether people form spatial indices and look at nothing when retrieving words from memory. Participants were simultaneously presented four nouns. Additionally, word presentation was sometimes followed by a visual cue either co-located (congruent) or not (incongruent) with the probe word. During retrieval, participants looked at the relevant, blank location, where the probe word had appeared previously, more than the other, irrelevant blank locations following a congruent visual cue and when there was no cue between encoding and retrieval (pure looking at nothing condition). Critically, participants with better visuospatial memory looked less at “nothing”, suggesting a dynamic relationship between so-called “external” and “internal” memory. Overall, findings suggest an automatic spatial indexing mechanism and a dynamic looking at nothing behaviour for words.</p>
<p><strong>Highlights</strong></p>
<ul>
<li>Participants offloaded memory work onto the environment with eye movements when remembering visually and simultaneously presented single words.</li>
<li>Worse visuospatial memory led to more reliance on the environment during retrieval.</li>
</ul>
</div>
<div id="introduction-1" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Introduction</h2>
<p>The human mind can anchor spatially-located information to external spatial locations. This mechanism has been expressed within a visual processing model, where the location of an object is separated from the visual features of it (Marr, 1982). This view, expanded into an exhaustive spatial indexing model (Pylyshyn, 1989), assumes that the visual system is able to individuate spatial relations before discerning a visual pattern and immediately index the locations of such patterns. In a similar fashion, spatial registration hypothesis (Coslett, 1999) holds that perceived stimuli are coded with respect to their location in space. Location, therefore, is a critical constituent of our interactions with the world (van der Heijden, 1993).
Within the spatial indexing (or spatial registering/encoding) model, spatial indices remain attached to a particular object independent of its movements and visual properties. Critically, spatiotemporal continuity (i.e., persistence of spatial “tags” over time) occurs even when the visual information disappears, as often manifested in mental imagery (e.g., Brandt &amp; Stark, 1997). Spatial indices tied to external visual and verbal information trigger eye movements when a mental representation is reactivated. Thus, when retrieving information from memory, people tend to exploit location-based indices and look at the seemingly uninformative, empty locations where the information originally occurred even if location is irrelevant to the task. This behaviour is known as looking at nothing (Spivey &amp; Geng, 2000).
In their pioneering study, Richardson and Spivey (2000) documented the use of spatial information and looking at nothing in verbal memory. Four faces randomly appeared on different quadrants of a two by two grid along with four corresponding spoken facts (e.g., “Shakespeare’s first plays were historical dramas; his last was the Tempest”). On the next screen, a statement (e.g., “Shakespeare’s first play was the Tempest”) probed participants’ memory for verbal information. During retrieval, there were significantly more looks in the blank quadrant where the face associated with the probed semantic information had been when compared to other quadrants. Thus, people did not just look at any nothing when answering the questions. Rather, they looked at an invisible spatial index, which was previously allocated to the information (Spivey &amp; Geng, 2000).
Looking at nothing may be best thought of as an interface between internal and external worlds. Ferreira, Apel and Henderson (2008) proposed an integrated memory architecture, where external cues and internal representations work hand in hand to retrieve information as efficiently as possible (see also Richardson, Altmann, Spivey, &amp; Hoover, 2009). More precisely, the integrated memory account combines visual/auditory and spatial information in the external world with visual, linguistic, spatial and conceptual counterparts in the mental world. When part of an integrated representation (linguistic information) is reactivated, the other parts (spatial information) are retrieved as well. In this regard, looking at nothing is also an example of spatial simulation (Barsalou, 1999) in that the spatial position where the information is presented is recreated when the information is needed again. Looking at nothing can also be thought as an example of efficient cognitive offloading (Risko &amp; Gilbert, 2016), in which the memory work is offloaded onto the world to minimise internal demands.
In the current study, we addressed the looking at nothing triangle, which is composed of actual looking behaviour, spatial indices and mental representations to answer three questions (1) How automatic is spatial indexing? Do individuals automatically index the locations of short and briefly presented linguistic information (e.g., visually and simultaneously presented single words)? (2) How dynamic is spatial indexing and looking at nothing? Can spatial indices be updated with subsequent visual information and how does it affect looking behaviour? (3) Does everybody look at blank locations, or is looking at nothing modulated by certain cognitive capacities such as visuospatial memory span?</p>
<div id="spatial-indexing-and-looking-at-nothing-automaticity" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Spatial indexing and looking at nothing: automaticity</h3>
<p>Looking at nothing typically occurs under two retrieval conditions as shown in the previous studies: (1) People look at blank locations when remembering spoken linguistic information such as factual sentences (Hoover &amp; Richardson, 2008; D. C. Richardson &amp; Kirkham, 2004; D. C. Richardson &amp; Spivey, 2000; Scholz et al., 2018, 2011, 2016). As illustrated above, spoken linguistic information is explicitly associated with a visual object in this paradigm, which we term as explicit indexing. In turn, eyes revisit the previous locations of the object (associated with the information) when retrieving the spoken factual information. (2) Looking at nothing also occurs during retrieval of visually presented non-linguistic information such as single objects (Martarelli &amp; Mast, 2013; Spivey &amp; Geng, 2000), arrangement of multiple objects (Altmann, 2004; Johansson &amp; Johansson, 2014) or visual patterns (Bochynska &amp; Laeng, 2015; Laeng et al., 2014). In this case, locations are encoded along with the visual object(s) or patterns.
In the current study, we adopted a different approach to examine the automaticity of spatial encoding of linguistic information. We showed participants four nouns on a grid simultaneously to study for a brief period of time. Then, an auditorily presented word (which could be either among the studied words or not) probed participants’ verbal recognition memory while participants were looking at a blank screen. If participants automatically encode location of the words as assumed in spatial indexing hypothesis (implicit indexing), they should display looking at nothing behaviour. In other words, we predict more fixations in the now-blank locations of the probe word during retrieval compared to the other, irrelevant blank locations. However, if explicit indexing is required for looking at nothing as shown in the previous studies, there should be same amount of spontaneous looks in the relevant and irrelevant blank locations.
Word locations are encoded in reading (Fischer, 1999; but Inhoff &amp; Weger, 2005), writing (Le Bigot, Passerault, &amp; Olive, 2009) and complex cognitive tasks such as memory-based decision-making (Jahn &amp; Braatz, 2014; Renkewitz &amp; Jahn, 2012; Scholz, von Helversen, &amp; Rieskamp, 2015). However, to what extent spatial encoding is automatic is not clear. An automatic process is fast, efficient, unconscious, unintentional, uncontrolled (i.e., cannot be wilfully inhibited), goal-independent and purely stimulus-driven (i.e., cannot be avoided) (Hasher &amp; Zacks, 1979; Moors &amp; De Houwer, 2006). Based on the criteria of automaticity, there is evidence that spatial encoding is an automatic process (Andrade &amp; Meudell, 1993), an effortful process (Naveh-Benjamin, 1987, 1988) and a combination of both (Ellis, 1991). For instance, Pezdek, Roman and Sobolik (1986) reported that spatial information for objects are more likely to be encoded automatically as compared to words.
If participants look at previous locations of the words they are asked to remember, this could provide evidence for the automaticity of spatial encoding in looking at nothing due to the specifics of the present experimental paradigm (i.e., implicit encoding and brief encoding time) and the nature of looking at nothing (i.e., an unintentional and efficient behaviour).</p>
</div>
<div id="spatial-indexing-and-looking-at-nothing-dynamicity" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Spatial indexing and looking at nothing: dynamicity</h3>
<p>How stable are the spatial indices in looking at nothing? Can they be updated with subsequent visuospatial information? How does updated spatial indices guide eye movements during memory retrieval? Answers to these questions are critical to understand mechanics of spatial indexing and looking at nothing. Thus, we tested whether congruency or incongruency of visuospatial cues between encoding and retrieval stages affects spatial indexing and looking at nothing.
There are studies examining the temporal stability of spatial indices. For example, in Wantz, Martarelli and Mast (2015), location memory for visual objects faded 24 hours after the initial encoding. In consequence, participants looked at relevant, blank locations immediately after the encoding, 5 minutes and 1 hour after the encoding but not after 24 hours. That said, less is known about the spatial stability of indices. In one study (D. C. Richardson &amp; Kirkham, 2004), looking at nothing was reported when the visual information that was associated with the to-be-retrieved information moved and thus, updated the spatial indices. Participants looked at the previous locations of the updated locations rather than the original locations of the previously encoded information, suggesting a flexible and a dynamic spatial indexing mechanism.
In the current study, a visual cue (i.e., a black dot) that was irrelevant to the words and to the task itself was shown between encoding and retrieval stages. The cue was presented either in the same quadrant of the grid or a diagonal quadrant as to the location of the probe word at the encoding stage. There was also a third condition, in which, the participants did not see a cue at all.
A plethora of studies on Simon effect (Simon &amp; Rudell, 1967) indicates that spatial congruency between the stimulus and the response results in faster and more accurate response even when the location is irrelevant to successful performance (see Hommel, 2011 for a review). In line, Vankov (2011) presented evidence for a Simon-like effect in spatial indexing and showed that compatibility of irrelevant spatial information benefits memory retrieval (see also Hommel, 2002; Wühr &amp; Ansorge, 2007; Zhang &amp; Johnson, 2004) Participants saw four objects on a 2 x 2 grid (e.g., a line drawing of a guitar, cat, camel and plane) at the encoding phase. Then, they were presented a word denoting either a new object or one of the studied objects (e.g., guitar) in one of the four locations as to the location of the target object; that is, in the same location, a vertical location (above or below the target object), a horizontal location (left or right of the target object) or a diagonal location. Participants were asked to remember whether the object denoted by the word appeared before. The fastest responses were found when the word cue appeared in the same location as to the target object. Participants were the slowest to respond when the word cue was in the diagonal location as to the target object.
In the light of the abovementioned evidence, we predict that (in)congruency between the spatial code attached to the word and the spatial code attached to the visuospatial cue could modulate looking at nothing behaviour. A congruent cue is predicted to emphasize the original location of the probe word and thus, the spatial indice tagging it. In turn, fixations to the relevant, blank locations should be more frequent in congruent cue condition as to no cue condition. On the other hand, an incongruent cue could update the spatial code attached to the word and disrupt looking to blank locations by shifting participants’ attention to a diagonal location. Such a pattern would suggest that spatial indexing and looking at nothing for words are dynamic processes that are sensitive to the systematic manipulation of irrelevant visuospatial information.</p>
</div>
<div id="looking-at-nothing-and-visuospatial-memory" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Looking at nothing and visuospatial memory</h3>
<p>The link between mental representations and looking at nothing is critical. One position within the radical grounded-embodied cognition (Chemero, 2011) is that the world functions as an outside memory without the need for mental representations (O’Regan, 1992). According to this view, the external memory store can be accessed at will through visual perception.
As discussed above, the integrated memory account (Ferreira et al., 2008) represents an opposing position within a relatively “traditional” grounded-embodied approach. Accordingly, “internal memory” (mental representations) and so called “external memory” (i.e., the external world internalised via spatial indices and eye movements) work cooperatively in an efficient and goal-directed manner in looking at nothing. To be more precise, “the opportunistic and efficient mind” (D. C. Richardson et al., 2009) exploits external support whenever it needs to minimise internal memory load. In support of this assumption, there is evidence that short-term memory capacity is a reliable predictor of conscious and intentional use of environment in memory tasks (see Risko &amp; Gilbert, 2016 for a review). In one memory study (Risko &amp; Dunn, 2015), offloading (i.e., writing down to-be-retrieved information) was given as an option to the participants. Results revealed that participants with worse short-term memory wrote down the information rather than relying on the internal memory more frequently than the participants with better short-term memory.
In looking at nothing, there is evidence that reliance on the environment increases/decreases in proportion to internal demands. For example, people tend to exhibit less looking at nothing as they are asked to study and recall the same sentences over and over again, suggesting less reliance on external cues as the task becomes easier through repetition (Scholz et al., 2011). Similarly, Wantz, Martarelli and Mast (2015) showed less looks to blank locations with repeated recall without rehearsal as mental representations stabilise in time.
However, not much is known about how individual differences in internal memory map onto the differences in looking at nothing within the scope of integrated memory account. If the opportunistic and efficient cognitive system uses both internal and external cues to access memory traces (D. C. Richardson et al., 2009) and if external cues are used to relieve internal operations (Risko &amp; Dunn, 2015), people with relatively worse visuospatial memory should rely more on the environment during memory retrieval (and vice versa). A correlation between visuospatial memory capacity and looking at nothing could provide further evidence for the integrated memory system by disproving <em>the world as an outside memory</em> argument (O’Regan, 1992) and consequently, radical grounded-embodied cognition.</p>
</div>
<div id="role-of-eye-movements-in-memory-retrieval" class="section level3" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Role of eye movements in memory retrieval</h3>
<p>Another fundamental issue is whether looks occur to blank regions that are associated with information facilitate the retrieval of this information. This issue taps into a seemingly simple question with regard to the very nature of memory-guided eye movements: Why do people look at nothing? Role and functionality of eye movements in memory retrieval have been highly controversial (see Ferreira et al., 2008; Mast &amp; Kosslyn, 2002; Richardson et al., 2009 for discussions). First studies did not present any evidence for improvement in memory with looks to blank spaces (Hoover &amp; Richardson, 2008; D. C. Richardson &amp; Kirkham, 2004; D. C. Richardson &amp; Spivey, 2000; Spivey &amp; Geng, 2000; Vankov, 2011). Initial failure to demonstrate memory enhancement lead to the preliminary conclusion that eye movements only co-assist the retrieval process as a by-product (Spivey, Richardson, &amp; Fitneva, 2004).
There is now growing evidence that gaze position can play a functional role in memory retrieval. For example, Laeng and Teodorescu (2002) reported that participants who viewed an image and looked at the blank screen freely (free perception &amp; free retrieval) were more accurate in answering the retrieval questions those whose gaze were restricted to the central fixation point (free perception &amp; fixed retrieval) (see also Johansson, Holsanova, Dewhurst, &amp; Holmqvist, 2012; Laeng et al., 2014 for memory advantage in free gaze compared to fixed gaze). In a similar gaze manipulation paradigm, participants who were instructed to look at relevant, blank regions were more accurate in judging statements about visual objects (Johansson &amp; Johansson, 2014) and verbal information (Scholz et al., 2018, 2016) than the participants who were instructed to look at a diagonal location as to the original location of the object or object associated with verbal information.
The current study was not designed to test the role of looking behaviour in memory. That is, eye gaze at retrieval was not manipulated as in the studies reviewed above. Rather, we analysed the functionality of looking at nothing by using the fixation percentage in the relevant quadrant (i.e., looking at nothing) as a predictor of hit rate and hit latency within mixed-effects models. If looks to the relevant, blank locations predict recognition memory for visually presented single words, it might provide tentative evidence for the facilitatory role of gaze position in memory.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="experiment-1.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
